{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial kernel based on different RNN layers\n",
    "\n",
    "* **Text preprocessing**\n",
    "\n",
    "Following things to be tried on the baseline:\n",
    "    * Add Early Stopping callback\n",
    "    * Increase max epochs - let EarlyStop do the work\n",
    "    * Add Tensorboard callback, monitor training\n",
    "    * Replace LSTM by GRU units and check if it changes anything\n",
    "    * Add another layer of LSTM/GRU, see if things improve\n",
    "    * Play around with Dense layers (add/# units/etc.)\n",
    "    * Find preprocessing rules you could add to improve the quality of the data\n",
    "    * Use different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Permute, GRU, Conv1D, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, CuDNNGRU, concatenate, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D, Dot\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from functools import reduce\n",
    "from keras.layers import Layer, PReLU\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "utility_path = '../utility/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'../utility/crawl-300d-2M.vec'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 200000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 200 # max number of words in a comment to use[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"comment_text\"] = train[\"comment_text\"].str.replace(\"[.?!]{1,}\\s\", \" <eos> \").str.replace(\"(\\n){1,}\", \" <eop> \").str.replace(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\")\n",
    "test[\"comment_text\"] = test[\"comment_text\"].str.replace(\"[.?!]{1,}\\s\", \" <eos> \").str.replace(\"(\\n){1,}\", \" <eop> \").replace(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\")\n",
    "\n",
    "train[\"comment_text\"] = train[\"comment_text\"].str.replace(\"\\n\", \"\")\n",
    "test[\"comment_text\"] = test[\"comment_text\"].str.replace(\"\\n\", \"\")\n",
    "\n",
    "#train[\"comment_text\"] = train[\"comment_text\"].str.replace(\"talk\", \"\").str.replace(\"==\", \"\")\n",
    "#test[\"comment_text\"] = test[\"comment_text\"].str.replace(\"talk\", \"\").str.replace(\"==\", \"\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Well <eos>  <eop> You could for instance beef up 'this with '''that <eos> Cheers,\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text.sample(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f9f45bb1ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0055286596, 0.34703913)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        # self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        # print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #print(mask.shape)\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        #print(mask.shape)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    #mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    #n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    # compute mask-aware mean of x\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False)\n",
    "    #print(x_mean.shape)\n",
    "    return x_mean\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  weights=[embedding_matrix],\n",
    "                 trainable=True, mask_zero=True)(inp)\n",
    "    #x = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\n",
    "    #x = Conv1D(filters=64, kernel_size=2)(x)\n",
    "    \n",
    "    #avg_emd = GlobalAveragePooling1D()(x)\n",
    "    #x = SpatialDropout1D(0.5)(x)\n",
    "    x = ZeroMaskedEntries()(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    #tmp = Bidirectional(CuDNNGRU(128, return_sequences=True, return_state=True))(x)\n",
    "    #x = tmp[0]\n",
    "    #state1 = tmp[1]\n",
    "    #state2 = tmp[2]\n",
    "    print(x)\n",
    "    x, state1 = CuDNNGRU(150, return_sequences=True, return_state=True)(x)\n",
    "    #x, state2 = CuDNNGRU(150, return_sequences=True, return_state=True)(x)\n",
    "    #x_flat = Flatten()(x)\n",
    "    #x_dot = GlobalAveragePooling1D()(x_dot)\n",
    "    #attn_pool = Permute((2,1))(x)\n",
    "    #attn_pool = Dense(maxlen, activation='tanh')(attn_pool)\n",
    "    #attn_pool = Dense(maxlen, activation=Activation(K.exp))(attn_pool)\n",
    "    #attn_pool = Permute((2,1))(attn_pool)\n",
    "    #attn_pool = GlobalAveragePooling1D()(attn_pool)\n",
    "    attn_pool = Attention(maxlen)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    \n",
    "\n",
    "    #x_dot = Dot(1)([avg_pool, max_pool])\n",
    "    x = concatenate([ max_pool, attn_pool, state1\n",
    "                    ])\n",
    "    #x = avg_pool\n",
    "    #x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(600)(x)\n",
    "    x = PReLU()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    #x = Dense(256)(x)\n",
    "    #x = PReLU()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    #x = Dense(6, activation='softmax')(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    opt = Adam(lr=0.001, decay=0, clipnorm=10)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['target_str'] = reduce(lambda x,y: x+y, [train[col].astype(str) for col in label_cols])\n",
    "train['target_str'] = train['target_str'].replace('110101', '000000').replace('110110','000000')\n",
    "cvlist = list(StratifiedShuffleSplit(n_splits=5, test_size=0.05, random_state=786).split(train, train['target_str'].astype('category')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"spatial_dropout1d_1/cond/Merge:0\", shape=(?, 200, 300), dtype=float32)\n",
      "Epoch 1/2\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9809\n",
      " ROC-AUC - epoch: 1 - score: 0.988923 \n",
      "\n",
      "151592/151592 [==============================] - 68s 447us/step - loss: 0.0521 - acc: 0.9809\n",
      "Epoch 2/2\n",
      "145536/151592 [===========================>..] - ETA: 2s - loss: 0.0333 - acc: 0.9864"
     ]
    }
   ],
   "source": [
    "def lr_decay(epoch):\n",
    "    if epoch == 0:\n",
    "        return 0.0015\n",
    "    if epoch == 1:\n",
    "        return 0.0008\n",
    "    if epoch == 2:\n",
    "        return 0.0008\n",
    "    if epoch == 3:\n",
    "        return 0.00001\n",
    "    \n",
    "y_preds = np.zeros((len(X_t), len(label_cols)))\n",
    "LRDecay = LearningRateScheduler(lr_decay)\n",
    "for tr_index, val_index in cvlist:\n",
    "    X_train, y_train = X_t[tr_index, :], y[tr_index]\n",
    "    X_val, y_val = X_t[val_index, :], y[val_index]\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    model = get_model()\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=2, validation_split=0.0, verbose=1, \n",
    "              callbacks=[RocAuc, LRDecay])\n",
    "    y_preds[val_index, :] = model.predict(X_val, batch_size=2048)\n",
    "    K.clear_session()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc_score(y, y_preds)\n",
    "import gc\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for i in range(4):\n",
    "    model = get_model()\n",
    "    model.fit(X_t, y, batch_size=128, epochs=2, validation_split=0.001, verbose=1, \n",
    "                  callbacks=[RocAuc, LRDecay])\n",
    "    y_test_preds1 = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "    test_preds.append(y_test_preds1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_mean = (test_preds[0] * test_preds[1] * test_preds[2] * test_preds[3])**(1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[label_cols] = test_preds_mean\n",
    "sample_submission.to_csv('nn_submission_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for class toxic is 0.9791088111561013\n",
      "Score for class severe_toxic is 0.9896361881404786\n",
      "Score for class obscene is 0.989792727484462\n",
      "Score for class threat is 0.9834375512104746\n",
      "Score for class insult is 0.9851961777705942\n",
      "Score for class identity_hate is 0.9837426177272286\n",
      "Over auc score 0.9851523455815565\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "y_trues = train[label_cols].values\n",
    "y_preds2 = np.zeros((X_t.shape[0], len(label_cols)))\n",
    "y_test_preds2 = np.zeros((X_te.shape[0], len(label_cols)))\n",
    "for i, col in enumerate(label_cols):\n",
    "    y = y_trues[:, i]\n",
    "    #model = RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_leaf=50, class_weight='balanced', n_jobs=-1)\n",
    "    model = lgb.LGBMClassifier(n_estimators=150, num_leaves=8, learning_rate=0.05, \n",
    "                               subsample=0.7, colsample_bytree=0.9)\n",
    "    y_preds2[:, i] = cross_val_predict(model, y_preds, y, cv=cvlist, n_jobs=1, method='predict_proba')[:,1]\n",
    "    y_test_preds2[:, i] = model.fit(y_preds, y).predict_proba(y_test_preds)[:,1]\n",
    "    print(\"Score for class {} is {}\".format(col, roc_auc_score(y, y_preds2[:, i])))\n",
    "print(\"Over auc score\", roc_auc_score(y_trues, y_preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[label_cols] = test_preds_mean\n",
    "sample_submission.to_csv('nn_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.998639</td>\n",
       "      <td>6.222732e-01</td>\n",
       "      <td>0.986446</td>\n",
       "      <td>5.110442e-02</td>\n",
       "      <td>9.549904e-01</td>\n",
       "      <td>5.519497e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.789467e-08</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.027940e-09</td>\n",
       "      <td>2.875290e-07</td>\n",
       "      <td>1.663085e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>3.100435e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>3.314079e-05</td>\n",
       "      <td>1.175709e-05</td>\n",
       "      <td>2.276327e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.953290e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.394860e-09</td>\n",
       "      <td>1.432248e-06</td>\n",
       "      <td>4.007862e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>7.988882e-07</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>5.988817e-07</td>\n",
       "      <td>3.980357e-06</td>\n",
       "      <td>2.081103e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat  \\\n",
       "0  00001cee341fdb12  0.998639  6.222732e-01  0.986446  5.110442e-02   \n",
       "1  0000247867823ef7  0.000003  3.789467e-08  0.000001  1.027940e-09   \n",
       "2  00013b17ad220c46  0.000370  3.100435e-06  0.000017  3.314079e-05   \n",
       "3  00017563c3f7919a  0.000001  2.953290e-07  0.000007  3.394860e-09   \n",
       "4  00017695ad8997eb  0.000042  7.988882e-07  0.000020  5.988817e-07   \n",
       "\n",
       "         insult  identity_hate  \n",
       "0  9.549904e-01   5.519497e-01  \n",
       "1  2.875290e-07   1.663085e-07  \n",
       "2  1.175709e-05   2.276327e-05  \n",
       "3  1.432248e-06   4.007862e-07  \n",
       "4  3.980357e-06   2.081103e-06  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[label_cols] = y_test_preds2\n",
    "sample_submission.to_csv('nn_lgbmeta_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
