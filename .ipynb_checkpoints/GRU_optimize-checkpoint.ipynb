{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial kernel based on different RNN layers\n",
    "\n",
    "* **Text preprocessing**\n",
    "\n",
    "Following things to be tried on the baseline:\n",
    "    * Add Early Stopping callback\n",
    "    * Increase max epochs - let EarlyStop do the work\n",
    "    * Add Tensorboard callback, monitor training\n",
    "    * Replace LSTM by GRU units and check if it changes anything\n",
    "    * Add another layer of LSTM/GRU, see if things improve\n",
    "    * Play around with Dense layers (add/# units/etc.)\n",
    "    * Find preprocessing rules you could add to improve the quality of the data\n",
    "    * Use different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Permute, GRU, Conv1D, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, CuDNNGRU, concatenate, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D, Dot\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from functools import reduce\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "utility_path = '../utility/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{utility_path}glove.42B.300d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 30000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, )\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0057200976, 0.29510665)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Do you want me to ban you? I am prejudice against people like you.\\nSo listen very carefully, do not fucking edit the page again or you will be banned''' If you or any other nazi has something to say, please send it to my talk. thank you.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.identity_hate == 1].comment_text.sample(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    185\n",
       "1      1\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.comment_text.str.contains(\"Please stop your disruptive editing\")].toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        # self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        # print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  weights=[embedding_matrix],\n",
    "                 trainable=True)(inp)\n",
    "    #x = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\n",
    "    #x = Conv1D(filters=64, kernel_size=2)(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    avg_emd = GlobalAveragePooling1D()(x)\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True, ))(x)\n",
    "    #x_flat = Flatten()(x)\n",
    "    #x_dot = GlobalAveragePooling1D()(x_dot)\n",
    "    #attn_pool = Permute((2,1))(x)\n",
    "    #attn_pool = Dense(maxlen, activation='tanh')(attn_pool)\n",
    "    #attn_pool = Dense(maxlen, activation=Activation(K.exp))(attn_pool)\n",
    "    #attn_pool = Permute((2,1))(attn_pool)\n",
    "    #attn_pool = GlobalAveragePooling1D()(attn_pool)\n",
    "    attn_pool = Attention(maxlen)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    \n",
    "\n",
    "    #x_dot = Dot(1)([avg_pool, max_pool])\n",
    "    x = concatenate([avg_pool, max_pool, attn_pool])\n",
    "    #x = avg_pool\n",
    "    #x = BatchNormalization()(x)\n",
    "    #x = Dense(32, activation=\"sigmoid\")(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    opt = Adam(lr=0.001, decay=0)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['target_str'] = reduce(lambda x,y: x+y, [train[col].astype(str) for col in label_cols])\n",
    "cvlist = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=786).split(train, train['target_str'].astype('category')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "143520/143600 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9815\n",
      " ROC-AUC - epoch: 1 - score: 0.986507 \n",
      "\n",
      "143600/143600 [==============================] - 77s 534us/step - loss: 0.0506 - acc: 0.9815\n",
      "Epoch 2/3\n",
      "143520/143600 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9852\n",
      " ROC-AUC - epoch: 2 - score: 0.986316 \n",
      "\n",
      "143600/143600 [==============================] - 76s 532us/step - loss: 0.0383 - acc: 0.9852\n",
      "Epoch 3/3\n",
      "143552/143600 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9882\n",
      " ROC-AUC - epoch: 3 - score: 0.986306 \n",
      "\n",
      "143600/143600 [==============================] - 76s 531us/step - loss: 0.0306 - acc: 0.9882\n",
      "Epoch 1/3\n",
      "143520/143606 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9817\n",
      " ROC-AUC - epoch: 1 - score: 0.985278 \n",
      "\n",
      "143606/143606 [==============================] - 76s 532us/step - loss: 0.0503 - acc: 0.9817\n",
      "Epoch 2/3\n",
      "143552/143606 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9851\n",
      " ROC-AUC - epoch: 2 - score: 0.984764 \n",
      "\n",
      "143606/143606 [==============================] - 76s 531us/step - loss: 0.0385 - acc: 0.9851\n",
      "Epoch 3/3\n",
      "143552/143606 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9879\n",
      " ROC-AUC - epoch: 3 - score: 0.985019 \n",
      "\n",
      "143606/143606 [==============================] - 76s 531us/step - loss: 0.0309 - acc: 0.9879\n",
      "Epoch 1/3\n",
      "143520/143611 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9818\n",
      " ROC-AUC - epoch: 1 - score: 0.986619 \n",
      "\n",
      "143611/143611 [==============================] - 76s 533us/step - loss: 0.0501 - acc: 0.9818\n",
      "Epoch 2/3\n",
      "143584/143611 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9851\n",
      " ROC-AUC - epoch: 2 - score: 0.987910 \n",
      "\n",
      "143611/143611 [==============================] - 76s 530us/step - loss: 0.0384 - acc: 0.9851\n",
      "Epoch 3/3\n",
      "143552/143611 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9883\n",
      " ROC-AUC - epoch: 3 - score: 0.987978 \n",
      "\n",
      "143611/143611 [==============================] - 76s 531us/step - loss: 0.0302 - acc: 0.9883\n",
      "Epoch 1/3\n",
      "143552/143602 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9815\n",
      " ROC-AUC - epoch: 1 - score: 0.984553 \n",
      "\n",
      "143602/143602 [==============================] - 76s 531us/step - loss: 0.0508 - acc: 0.9815\n",
      "Epoch 2/3\n",
      "143520/143602 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.984922 \n",
      "\n",
      "143602/143602 [==============================] - 76s 529us/step - loss: 0.0382 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "143520/143602 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9879\n",
      " ROC-AUC - epoch: 3 - score: 0.984812 \n",
      "\n",
      "143602/143602 [==============================] - 76s 530us/step - loss: 0.0308 - acc: 0.9879\n",
      "Epoch 1/3\n",
      "143520/143608 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9811\n",
      " ROC-AUC - epoch: 1 - score: 0.986150 \n",
      "\n",
      "143608/143608 [==============================] - 76s 532us/step - loss: 0.0513 - acc: 0.9811\n",
      "Epoch 2/3\n",
      "143520/143608 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9849\n",
      " ROC-AUC - epoch: 2 - score: 0.986962 \n",
      "\n",
      "143608/143608 [==============================] - 76s 532us/step - loss: 0.0385 - acc: 0.9849\n",
      "Epoch 3/3\n",
      "143552/143608 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9882\n",
      " ROC-AUC - epoch: 3 - score: 0.986934 \n",
      "\n",
      "143608/143608 [==============================] - 76s 530us/step - loss: 0.0305 - acc: 0.9882\n",
      "Epoch 1/3\n",
      "143584/143607 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9817\n",
      " ROC-AUC - epoch: 1 - score: 0.987511 \n",
      "\n",
      "143607/143607 [==============================] - 77s 534us/step - loss: 0.0508 - acc: 0.9817\n",
      "Epoch 2/3\n",
      "143520/143607 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.987776 \n",
      "\n",
      "143607/143607 [==============================] - 76s 530us/step - loss: 0.0385 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "143552/143607 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9881\n",
      " ROC-AUC - epoch: 3 - score: 0.987889 \n",
      "\n",
      "143607/143607 [==============================] - 76s 530us/step - loss: 0.0306 - acc: 0.9881\n",
      "Epoch 1/3\n",
      "143616/143622 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9815\n",
      " ROC-AUC - epoch: 1 - score: 0.987103 \n",
      "\n",
      "143622/143622 [==============================] - 77s 533us/step - loss: 0.0507 - acc: 0.9815\n",
      "Epoch 2/3\n",
      "143584/143622 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.987145 \n",
      "\n",
      "143622/143622 [==============================] - 76s 528us/step - loss: 0.0384 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "143584/143622 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9880\n",
      " ROC-AUC - epoch: 3 - score: 0.987339 \n",
      "\n",
      "143622/143622 [==============================] - 76s 529us/step - loss: 0.0309 - acc: 0.9880\n",
      "Epoch 1/3\n",
      "143520/143622 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9816\n",
      " ROC-AUC - epoch: 1 - score: 0.985343 \n",
      "\n",
      "143622/143622 [==============================] - 76s 532us/step - loss: 0.0507 - acc: 0.9816\n",
      "Epoch 2/3\n",
      "143616/143622 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9851\n",
      " ROC-AUC - epoch: 2 - score: 0.984579 \n",
      "\n",
      "143622/143622 [==============================] - 76s 531us/step - loss: 0.0385 - acc: 0.9851\n",
      "Epoch 3/3\n",
      "143616/143622 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9880\n",
      " ROC-AUC - epoch: 3 - score: 0.984241 \n",
      "\n",
      "143622/143622 [==============================] - 76s 531us/step - loss: 0.0307 - acc: 0.9880\n",
      "Epoch 1/3\n",
      "143584/143630 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9815\n",
      " ROC-AUC - epoch: 1 - score: 0.985466 \n",
      "\n",
      "143630/143630 [==============================] - 76s 531us/step - loss: 0.0506 - acc: 0.9815\n",
      "Epoch 2/3\n",
      "143584/143630 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.983921 \n",
      "\n",
      "143630/143630 [==============================] - 76s 530us/step - loss: 0.0386 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "143616/143630 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9880\n",
      " ROC-AUC - epoch: 3 - score: 0.983751 \n",
      "\n",
      "143630/143630 [==============================] - 76s 531us/step - loss: 0.0307 - acc: 0.9880\n",
      "Epoch 1/3\n",
      "143616/143631 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9816\n",
      " ROC-AUC - epoch: 1 - score: 0.988734 \n",
      "\n",
      "143631/143631 [==============================] - 76s 532us/step - loss: 0.0508 - acc: 0.9816\n",
      "Epoch 2/3\n",
      "143584/143631 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.988810 \n",
      "\n",
      "143631/143631 [==============================] - 76s 530us/step - loss: 0.0387 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "143520/143631 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9879\n",
      " ROC-AUC - epoch: 3 - score: 0.989065 \n",
      "\n",
      "143631/143631 [==============================] - 76s 528us/step - loss: 0.0310 - acc: 0.9879\n"
     ]
    }
   ],
   "source": [
    "def lr_decay(epoch):\n",
    "    if epoch == 0:\n",
    "        return 0.001\n",
    "    if epoch == 1:\n",
    "        return 0.001\n",
    "    if epoch == 2:\n",
    "        return 0.00001\n",
    "    if epoch == 3:\n",
    "        return 0.00001\n",
    "    \n",
    "y_preds = np.zeros((len(X_t), len(label_cols)))\n",
    "LRDecay = LearningRateScheduler(lr_decay)\n",
    "for tr_index, val_index in cvlist:\n",
    "    X_train, y_train = X_t[tr_index, :], y[tr_index]\n",
    "    X_val, y_val = X_t[val_index, :], y[val_index]\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    model = get_model()\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=3, validation_split=0.0, verbose=1, \n",
    "              callbacks=[RocAuc, LRDecay])\n",
    "    y_preds[val_index, :] = model.predict(X_val, batch_size=2048)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861737784746518"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "159456/159571 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9818\n",
      " ROC-AUC - epoch: 1 - score: 0.992135 \n",
      "\n",
      "159571/159571 [==============================] - 84s 528us/step - loss: 0.0498 - acc: 0.9818\n",
      "Epoch 2/3\n",
      "159488/159571 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850\n",
      " ROC-AUC - epoch: 2 - score: 0.995249 \n",
      "\n",
      "159571/159571 [==============================] - 83s 523us/step - loss: 0.0384 - acc: 0.9850\n",
      "Epoch 3/3\n",
      "159488/159571 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9880\n",
      " ROC-AUC - epoch: 3 - score: 0.995429 \n",
      "\n",
      "159571/159571 [==============================] - 84s 525us/step - loss: 0.0307 - acc: 0.9880\n",
      "153164/153164 [==============================] - 2s 15us/step\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(X_t, y, batch_size=32, epochs=3, validation_split=0.0, verbose=1, \n",
    "              callbacks=[RocAuc, LRDecay])\n",
    "y_test_preds = model.predict([X_te], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for class toxic is 0.9791088111561013\n",
      "Score for class severe_toxic is 0.9896361881404786\n",
      "Score for class obscene is 0.989792727484462\n",
      "Score for class threat is 0.9834375512104746\n",
      "Score for class insult is 0.9851961777705942\n",
      "Score for class identity_hate is 0.9837426177272286\n",
      "Over auc score 0.9851523455815565\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "y_trues = train[label_cols].values\n",
    "y_preds2 = np.zeros((X_t.shape[0], len(label_cols)))\n",
    "y_test_preds2 = np.zeros((X_te.shape[0], len(label_cols)))\n",
    "for i, col in enumerate(label_cols):\n",
    "    y = y_trues[:, i]\n",
    "    #model = RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_leaf=50, class_weight='balanced', n_jobs=-1)\n",
    "    model = lgb.LGBMClassifier(n_estimators=100, num_leaves=5, learning_rate=0.03, \n",
    "                               subsample=0.9, colsample_bytree=0.9)\n",
    "    y_preds2[:, i] = cross_val_predict(model, y_preds, y, cv=cvlist, n_jobs=1, method='predict_proba')[:,1]\n",
    "    y_test_preds2[:, i] = model.fit(y_preds, y).predict_proba(y_test_preds)[:,1]\n",
    "    print(\"Score for class {} is {}\".format(col, roc_auc_score(y, y_preds2[:, i])))\n",
    "print(\"Over auc score\", roc_auc_score(y_trues, y_preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[label_cols] = y_test_preds\n",
    "sample_submission.to_csv('nn_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[label_cols] = y_test_preds2\n",
    "sample_submission.to_csv('nn_lgbmeta_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
