{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-GRNN\n",
    "\n",
    "Implementation based on paper here:\n",
    "http://aclweb.org/anthology/D15-1167\n",
    "\n",
    "Inspiration for code taken from here:\n",
    "https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, gc, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Permute, GRU, Conv1D, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, CuDNNGRU, concatenate, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D, Dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from functools import reduce\n",
    "from keras.layers import Layer, PReLU, SpatialDropout1D\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TweetTokenizer, MWETokenizer, ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "np.random.seed(786)\n",
    "\n",
    "from Tokenizer import Tokenizer\n",
    "from ZeroMaskedLayer import ZeroMaskedLayer\n",
    "from AttentionLayer import AttentionLayer\n",
    "from RocAucEvaluation import RocAucEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "utility_path = '../utility/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{utility_path}crawl-300d-2M.vec'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(series):\n",
    "    return series.apply(lambda s: unicodedata.normalize('NFKC', str(s)))\n",
    "\n",
    "STOP_WORDS = set(stopwords.words( 'english' ))\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    #\"m\": \"am\",\n",
    "    #\"r\": \"are\",\n",
    "    #\"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "}\n",
    "\n",
    "#https://stackoverflow.com/questions/15175142/how-can-i-do-multiple-substitutions-using-regex-in-python\n",
    "def one_xlat(match):\n",
    "        return repl[match.group(0)]\n",
    "    \n",
    "rx = re.compile('|'.join(map(re.escape, repl)))\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(series):\n",
    "    series = unicodeToAscii(series)\n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace(rx, one_xlat)\n",
    "    series = series.str.replace(r\"(\\n){1,}\", \" \")\n",
    "    series = series.str.replace(r\"\\'\", \"\")\n",
    "    series = series.str.replace(r\"\\-\", \"\")\n",
    "    series = series.str.replace(r\"[^0-9a-zA-Z.,!?]+\", \" \")\n",
    "    series = series.str.replace(r\"[.]+\",\".\")\n",
    "    series = series.str.replace(r\"[!]+\",\"!\")\n",
    "    series = series.str.replace(r\"[?]+\",\".\")\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (153164, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "#Get validation folds\n",
    "train['target_str'] = reduce(lambda x,y: x+y, [train[col].astype(str) for col in list_classes])\n",
    "train['target_str'] = train['target_str'].replace('110101', '000000').replace('110110','000000')\n",
    "cvlist1 = list(StratifiedKFold(n_splits=10, random_state=786).split(train, train['target_str'].astype('category')))\n",
    "cvlist2 = list(StratifiedShuffleSplit(n_splits=5, test_size=0.05, random_state=786).split(train, train['target_str'].astype('category')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
